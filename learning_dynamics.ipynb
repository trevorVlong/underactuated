{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "name": "Copy of learning_dynamics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trevorVlong/underactuated/blob/main/learning_dynamics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kiuxBR04DcZ"
      },
      "source": [
        "# Dynamics Regression and Graphical Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpqbPGN731Z8"
      },
      "source": [
        "## Notebook Setup \n",
        "The following cell will checkout the underactuated repository, and set up the path (only if necessary).\n",
        "- On Google's Colaboratory, this **will take approximately two minutes** on the first time it runs (to provision the machine), but should only need to reinstall once every 12 hours.  Colab will ask you to \"Reset all runtimes\"; say no to save yourself the reinstall.\n",
        "- On Binder, the machines should already be provisioned by the time you can run this; it should return (almost) instantly.\n",
        "\n",
        "More details are available [here](http://underactuated.mit.edu/drake.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvu5-Bm531Z-"
      },
      "source": [
        "import importlib\n",
        "import os, sys\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "# Install drake (and underactuated).\n",
        "if 'google.colab' in sys.modules and importlib.util.find_spec('underactuated') is None:\n",
        "    urlretrieve(f\"http://underactuated.csail.mit.edu/scripts/setup/setup_underactuated_colab.py\",\n",
        "                \"setup_underactuated_colab.py\")\n",
        "    from setup_underactuated_colab import setup_underactuated\n",
        "    setup_underactuated(underactuated_sha='1dca1b915977aeec9f327d61aaeac4cfb9c6b408', drake_version='0.25.0', drake_build='releases')\n",
        "\n",
        "server_args = []\n",
        "if 'google.colab' in sys.modules:\n",
        "  server_args = ['--ngrok_http_tunnel']\n",
        "\n",
        "# Setup matplotlib.  \n",
        "from IPython import get_ipython\n",
        "if get_ipython() is not None: get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
        "\n",
        "# python libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# underactuated imports\n",
        "from underactuated import plot_2d_phase_portrait"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fjIsN4431aA"
      },
      "source": [
        "## Problem Description\n",
        "In this problem you will implement a simple neural network in pytorch to model the dynamics of the damped pendulum seen in class from data. At the end of the notebook, you will perform a graphical analysis and answer a few questions about the system.\n",
        "\n",
        "**These are the main steps of the exercise:**\n",
        "1. Implement the network architecture in pytorch\n",
        "2. Perform a graphical analysis of the dynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hNO98ajN_z1"
      },
      "source": [
        "## Sampling measurements\n",
        "In class, we derived the dynamics for the damped pendulum system analytically:\n",
        "\n",
        "> $b\\dot\\theta = u_0 - mgl\\sin\\theta$\n",
        "\n",
        "In this problem, we pretend we don't know the relationship $\\dot \\theta = f(\\theta)$ and would like to learn it from $(\\theta, \\dot \\theta)$ data measurements. That is, we'd like to train a neural network $NN(\\theta) \\approx f(\\theta) = \\dot \\theta$.\n",
        "\n",
        "In the cell below, we assume a pendulum with $l = 1$ and $m = 1$ for simplicity. Gravity is $g = 9.81$, and we assume 0 torque. We generate 10000 evenly spaced samples from the ground truth equations, and then sample 100 examples and add some random noise to \"simulate\" taking 100 measurements. The data we're trying to fit is plotted below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akkyz7AzRTta"
      },
      "source": [
        "### Pytorch tip\n",
        "\n",
        "One tip you'll find useful when debugging is to fix the random seed generators for your machine learning library (in our case pytorch), and potentially for numpy as well. This allows us to ensure that our neural network will be initialized with the same values each time we run our code, and it will allow us to see the effects of our changes and make things easier to debug. We can fix the random generator seed as seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuUod6sRZG68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "dd6d8367-7951-4aad-c97c-fac3e0e06f07"
      },
      "source": [
        "np.random.seed(5) # you can pick any integer for different seeds\n",
        "torch.manual_seed(5)\n",
        "\n",
        "# System parameters. Feel free to play with these, but use\n",
        "# the original set for the autograder!\n",
        "m = 1\n",
        "l = 1\n",
        "g = 9.81\n",
        "u = 0\n",
        "\n",
        "def generate_measurements():\n",
        "  # create ground truth data points\n",
        "  theta = torch.unsqueeze(torch.linspace(-2*np.pi, 2*np.pi, 10000), dim=1)\n",
        "\n",
        "  # subsample 100 ground truth data points and add noise\n",
        "  idxs = np.random.choice(10000, 100)\n",
        "  theta = theta[idxs]\n",
        "  theta_dot = u - m*g*l*torch.sin(theta) + 1.5*torch.rand(theta.size()) - 1.5*torch.rand(theta.size())\n",
        "\n",
        "  return theta, theta_dot\n",
        "\n",
        "# plot sampled measurements\n",
        "theta, theta_dot = generate_measurements()\n",
        "plt.scatter(theta, theta_dot, s=5)\n",
        "plt.xlabel('theta')\n",
        "plt.ylabel('theta_dot')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'theta_dot')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N_XNXq0QveP"
      },
      "source": [
        "## Defining our Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz3sl9MCSCAd"
      },
      "source": [
        "We will now implement our neural network in pytorch. Pytorch allows us to define a model class as a subclass of `nn.Module` (checkout the [pytorch docs](https://pytorch.org/docs/stable/index.html) for more information). We define our network architecture at initialization, and then we define a `forward` method that defines our forward pass. This method takes in the inputs to our neural network, and returns the outputs. \n",
        "\n",
        "We've provided an example model that shows you how to stack two linear layers with a Leaky ReLU nonlinearity in between them. Your job is to define `self.model`.\n",
        "\n",
        "- Layer 1: linear layer with 1 input and 200 outputs\n",
        "- Leaky ReLU nonlinearity\n",
        "- Layer 2: linear layer with 200 inputs and 100 outputs\n",
        "- Leaky ReLU nonlinearity\n",
        "- Layer 3: linear layer with 100 inputs and 1 output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4didPA2EDhN"
      },
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Network, self).__init__()\n",
        "\n",
        "    # Example model\n",
        "    self.example_model = nn.Sequential(\n",
        "        torch.nn.Linear(1, 50),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(50, 1)\n",
        "    )\n",
        "    \n",
        "    ####################################\n",
        "    # Define your model\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        torch.nn.Linear(1,200),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(200,100),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(100,1)\n",
        "    )\n",
        "    \n",
        "    ####################################\n",
        "\n",
        "  def forward(self, theta):\n",
        "    theta_dot_hat = self.model(theta)\n",
        "    return theta_dot_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5soWOWETglM"
      },
      "source": [
        "## Training our Network\n",
        "\n",
        "Now we can instantiate a network, an optimizing algorithm that will update the weights of our network, and a loss function. We will be using a mean-squared error loss for this problem. We are going to use the following optimizer and loss function:\n",
        "\n",
        "- Optimizer: [Adam optimizer](https://pytorch.org/docs/stable/optim.html)\n",
        "- Loss function: [Mean-squared Error loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)\n",
        "\n",
        "In the cell below, your job is to instantiate a mean-squared error loss, and an Adam optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMfELFEPVImq"
      },
      "source": [
        "We then iteratively update the parameters of our network. This is done by feeding in the theta values we measured, and computing a loss using the outputs (the networks \"guess\" for the theta dot outputs). If our predicted theta dots are far from the theta dots we measured, then our loss will be high and vice versa. We use the loss to call `loss.backward()`, pytorch runs [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) to update the network weights we passed into the Adam optimizer at initialization for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyiw_7dVyeUZ"
      },
      "source": [
        "### Pytorch tip\n",
        "\n",
        "Note that we zero out our gradients before we step through our optimizer at each iteration. We need to do this because pytorch accumulates gradients after each pass. This is used to train other types of networks like RNN's, but for our purposes here we want just the new gradients at each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EufqWTI0FjYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e76f4d5f-81e0-46ea-bea1-4ecf50a4538a"
      },
      "source": [
        "np.random.seed(5) # you can pick any integer for different seeds\n",
        "torch.manual_seed(5)\n",
        "\n",
        "# Feel free to play with these, but keep the original values for the autograder.\n",
        "iterations = 1000 # number of optimization iterations\n",
        "learning_rate = .05 # you do not need to tune this!\n",
        "\n",
        "# Generate measurements\n",
        "theta, theta_dot = generate_measurements()\n",
        "\n",
        "# Initialize our network, optimizer, and loss function\n",
        "NN = Network()\n",
        "\n",
        "####################################\n",
        "# Instantiate a mean-squared error loss\n",
        "# and an Adam optimizer. You can access\n",
        "# your networks parameters with NN.parameters().\n",
        "# You will need to pass this in as an\n",
        "# argument when instantiating the optimizer.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "optimizer = torch.optim.Adam(NN.parameters())\n",
        "L = torch.nn.MSELoss()\n",
        "\n",
        "####################################\n",
        "\n",
        "final_loss = 0.0\n",
        "if optimizer is not None and L is not None:\n",
        "  for t in range(iterations):\n",
        "\n",
        "    theta_dot_hat = NN(theta) # pass data through your neural net, and generate x_ddot predictions\n",
        "\n",
        "    loss = L(theta_dot_hat, theta_dot) # compare your predictions with the measured x_ddot values from your data set\n",
        "\n",
        "    print('Loss @ time {}: {}'.format(t, loss.item()))\n",
        "\n",
        "    optimizer.zero_grad() # clear gradients from the last iteration, before generating gradients for this iteration\n",
        "    loss.backward() # run backpropagation, and compute gradients\n",
        "    optimizer.step() # apply gradients to your network weights\n",
        "\n",
        "  final_loss = loss.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss @ time 0: 48.07992172241211\n",
            "Loss @ time 1: 45.680519104003906\n",
            "Loss @ time 2: 43.8422966003418\n",
            "Loss @ time 3: 42.37051773071289\n",
            "Loss @ time 4: 41.235836029052734\n",
            "Loss @ time 5: 40.38322067260742\n",
            "Loss @ time 6: 39.70164489746094\n",
            "Loss @ time 7: 39.133453369140625\n",
            "Loss @ time 8: 38.6715087890625\n",
            "Loss @ time 9: 38.29315948486328\n",
            "Loss @ time 10: 37.976600646972656\n",
            "Loss @ time 11: 37.7104377746582\n",
            "Loss @ time 12: 37.493621826171875\n",
            "Loss @ time 13: 37.292423248291016\n",
            "Loss @ time 14: 37.07884216308594\n",
            "Loss @ time 15: 36.843048095703125\n",
            "Loss @ time 16: 36.566890716552734\n",
            "Loss @ time 17: 36.25083541870117\n",
            "Loss @ time 18: 35.90122985839844\n",
            "Loss @ time 19: 35.529563903808594\n",
            "Loss @ time 20: 35.147762298583984\n",
            "Loss @ time 21: 34.767250061035156\n",
            "Loss @ time 22: 34.394500732421875\n",
            "Loss @ time 23: 34.033546447753906\n",
            "Loss @ time 24: 33.68429946899414\n",
            "Loss @ time 25: 33.34605407714844\n",
            "Loss @ time 26: 33.01543045043945\n",
            "Loss @ time 27: 32.688846588134766\n",
            "Loss @ time 28: 32.36116027832031\n",
            "Loss @ time 29: 32.03035354614258\n",
            "Loss @ time 30: 31.691221237182617\n",
            "Loss @ time 31: 31.341655731201172\n",
            "Loss @ time 32: 30.980749130249023\n",
            "Loss @ time 33: 30.60924530029297\n",
            "Loss @ time 34: 30.23056411743164\n",
            "Loss @ time 35: 29.847152709960938\n",
            "Loss @ time 36: 29.46160316467285\n",
            "Loss @ time 37: 29.075624465942383\n",
            "Loss @ time 38: 28.690610885620117\n",
            "Loss @ time 39: 28.307167053222656\n",
            "Loss @ time 40: 27.925657272338867\n",
            "Loss @ time 41: 27.545820236206055\n",
            "Loss @ time 42: 27.166244506835938\n",
            "Loss @ time 43: 26.785873413085938\n",
            "Loss @ time 44: 26.40374755859375\n",
            "Loss @ time 45: 26.02045249938965\n",
            "Loss @ time 46: 25.63812255859375\n",
            "Loss @ time 47: 25.257863998413086\n",
            "Loss @ time 48: 24.880046844482422\n",
            "Loss @ time 49: 24.506103515625\n",
            "Loss @ time 50: 24.137067794799805\n",
            "Loss @ time 51: 23.773765563964844\n",
            "Loss @ time 52: 23.4168643951416\n",
            "Loss @ time 53: 23.063514709472656\n",
            "Loss @ time 54: 22.71521759033203\n",
            "Loss @ time 55: 22.373573303222656\n",
            "Loss @ time 56: 22.03662109375\n",
            "Loss @ time 57: 21.705724716186523\n",
            "Loss @ time 58: 21.381786346435547\n",
            "Loss @ time 59: 21.065711975097656\n",
            "Loss @ time 60: 20.757226943969727\n",
            "Loss @ time 61: 20.4555721282959\n",
            "Loss @ time 62: 20.161849975585938\n",
            "Loss @ time 63: 19.875383377075195\n",
            "Loss @ time 64: 19.59521484375\n",
            "Loss @ time 65: 19.32263946533203\n",
            "Loss @ time 66: 19.058155059814453\n",
            "Loss @ time 67: 18.802135467529297\n",
            "Loss @ time 68: 18.553543090820312\n",
            "Loss @ time 69: 18.314950942993164\n",
            "Loss @ time 70: 18.083843231201172\n",
            "Loss @ time 71: 17.862028121948242\n",
            "Loss @ time 72: 17.647878646850586\n",
            "Loss @ time 73: 17.43829345703125\n",
            "Loss @ time 74: 17.23482322692871\n",
            "Loss @ time 75: 17.038389205932617\n",
            "Loss @ time 76: 16.84937286376953\n",
            "Loss @ time 77: 16.665565490722656\n",
            "Loss @ time 78: 16.486177444458008\n",
            "Loss @ time 79: 16.312925338745117\n",
            "Loss @ time 80: 16.144088745117188\n",
            "Loss @ time 81: 15.979432106018066\n",
            "Loss @ time 82: 15.819803237915039\n",
            "Loss @ time 83: 15.663501739501953\n",
            "Loss @ time 84: 15.510551452636719\n",
            "Loss @ time 85: 15.362237930297852\n",
            "Loss @ time 86: 15.217917442321777\n",
            "Loss @ time 87: 15.078180313110352\n",
            "Loss @ time 88: 14.942800521850586\n",
            "Loss @ time 89: 14.811601638793945\n",
            "Loss @ time 90: 14.685502052307129\n",
            "Loss @ time 91: 14.563908576965332\n",
            "Loss @ time 92: 14.446863174438477\n",
            "Loss @ time 93: 14.334338188171387\n",
            "Loss @ time 94: 14.226734161376953\n",
            "Loss @ time 95: 14.123461723327637\n",
            "Loss @ time 96: 14.025339126586914\n",
            "Loss @ time 97: 13.930950164794922\n",
            "Loss @ time 98: 13.840564727783203\n",
            "Loss @ time 99: 13.75478744506836\n",
            "Loss @ time 100: 13.673235893249512\n",
            "Loss @ time 101: 13.594825744628906\n",
            "Loss @ time 102: 13.520267486572266\n",
            "Loss @ time 103: 13.450126647949219\n",
            "Loss @ time 104: 13.383410453796387\n",
            "Loss @ time 105: 13.320160865783691\n",
            "Loss @ time 106: 13.259551048278809\n",
            "Loss @ time 107: 13.201371192932129\n",
            "Loss @ time 108: 13.146201133728027\n",
            "Loss @ time 109: 13.094893455505371\n",
            "Loss @ time 110: 13.046095848083496\n",
            "Loss @ time 111: 12.998862266540527\n",
            "Loss @ time 112: 12.953869819641113\n",
            "Loss @ time 113: 12.910883903503418\n",
            "Loss @ time 114: 12.868582725524902\n",
            "Loss @ time 115: 12.828664779663086\n",
            "Loss @ time 116: 12.789947509765625\n",
            "Loss @ time 117: 12.751988410949707\n",
            "Loss @ time 118: 12.715638160705566\n",
            "Loss @ time 119: 12.68002700805664\n",
            "Loss @ time 120: 12.644705772399902\n",
            "Loss @ time 121: 12.610630989074707\n",
            "Loss @ time 122: 12.577571868896484\n",
            "Loss @ time 123: 12.545007705688477\n",
            "Loss @ time 124: 12.513415336608887\n",
            "Loss @ time 125: 12.48205852508545\n",
            "Loss @ time 126: 12.450765609741211\n",
            "Loss @ time 127: 12.420495986938477\n",
            "Loss @ time 128: 12.390388488769531\n",
            "Loss @ time 129: 12.360352516174316\n",
            "Loss @ time 130: 12.330341339111328\n",
            "Loss @ time 131: 12.301078796386719\n",
            "Loss @ time 132: 12.271797180175781\n",
            "Loss @ time 133: 12.242501258850098\n",
            "Loss @ time 134: 12.213654518127441\n",
            "Loss @ time 135: 12.184796333312988\n",
            "Loss @ time 136: 12.155828475952148\n",
            "Loss @ time 137: 12.126944541931152\n",
            "Loss @ time 138: 12.098219871520996\n",
            "Loss @ time 139: 12.069677352905273\n",
            "Loss @ time 140: 12.041126251220703\n",
            "Loss @ time 141: 12.01231575012207\n",
            "Loss @ time 142: 11.983463287353516\n",
            "Loss @ time 143: 11.954995155334473\n",
            "Loss @ time 144: 11.925759315490723\n",
            "Loss @ time 145: 11.897019386291504\n",
            "Loss @ time 146: 11.868563652038574\n",
            "Loss @ time 147: 11.839366912841797\n",
            "Loss @ time 148: 11.81029987335205\n",
            "Loss @ time 149: 11.781344413757324\n",
            "Loss @ time 150: 11.751903533935547\n",
            "Loss @ time 151: 11.722295761108398\n",
            "Loss @ time 152: 11.693696022033691\n",
            "Loss @ time 153: 11.664281845092773\n",
            "Loss @ time 154: 11.633871078491211\n",
            "Loss @ time 155: 11.604135513305664\n",
            "Loss @ time 156: 11.574441909790039\n",
            "Loss @ time 157: 11.544172286987305\n",
            "Loss @ time 158: 11.513453483581543\n",
            "Loss @ time 159: 11.482895851135254\n",
            "Loss @ time 160: 11.452558517456055\n",
            "Loss @ time 161: 11.421380996704102\n",
            "Loss @ time 162: 11.390499114990234\n",
            "Loss @ time 163: 11.359569549560547\n",
            "Loss @ time 164: 11.328264236450195\n",
            "Loss @ time 165: 11.2966947555542\n",
            "Loss @ time 166: 11.265437126159668\n",
            "Loss @ time 167: 11.233940124511719\n",
            "Loss @ time 168: 11.202122688293457\n",
            "Loss @ time 169: 11.170079231262207\n",
            "Loss @ time 170: 11.137889862060547\n",
            "Loss @ time 171: 11.106012344360352\n",
            "Loss @ time 172: 11.073219299316406\n",
            "Loss @ time 173: 11.040580749511719\n",
            "Loss @ time 174: 11.007884979248047\n",
            "Loss @ time 175: 10.975303649902344\n",
            "Loss @ time 176: 10.942132949829102\n",
            "Loss @ time 177: 10.908895492553711\n",
            "Loss @ time 178: 10.875484466552734\n",
            "Loss @ time 179: 10.842066764831543\n",
            "Loss @ time 180: 10.808337211608887\n",
            "Loss @ time 181: 10.774236679077148\n",
            "Loss @ time 182: 10.740443229675293\n",
            "Loss @ time 183: 10.706006050109863\n",
            "Loss @ time 184: 10.67148208618164\n",
            "Loss @ time 185: 10.637042045593262\n",
            "Loss @ time 186: 10.602130889892578\n",
            "Loss @ time 187: 10.566828727722168\n",
            "Loss @ time 188: 10.531261444091797\n",
            "Loss @ time 189: 10.496208190917969\n",
            "Loss @ time 190: 10.459997177124023\n",
            "Loss @ time 191: 10.424114227294922\n",
            "Loss @ time 192: 10.387982368469238\n",
            "Loss @ time 193: 10.351799011230469\n",
            "Loss @ time 194: 10.315617561340332\n",
            "Loss @ time 195: 10.278675079345703\n",
            "Loss @ time 196: 10.241924285888672\n",
            "Loss @ time 197: 10.20523738861084\n",
            "Loss @ time 198: 10.168014526367188\n",
            "Loss @ time 199: 10.130819320678711\n",
            "Loss @ time 200: 10.093637466430664\n",
            "Loss @ time 201: 10.056099891662598\n",
            "Loss @ time 202: 10.018122673034668\n",
            "Loss @ time 203: 9.979994773864746\n",
            "Loss @ time 204: 9.94177532196045\n",
            "Loss @ time 205: 9.903610229492188\n",
            "Loss @ time 206: 9.865131378173828\n",
            "Loss @ time 207: 9.826546669006348\n",
            "Loss @ time 208: 9.787677764892578\n",
            "Loss @ time 209: 9.749180793762207\n",
            "Loss @ time 210: 9.710280418395996\n",
            "Loss @ time 211: 9.671055793762207\n",
            "Loss @ time 212: 9.631712913513184\n",
            "Loss @ time 213: 9.592460632324219\n",
            "Loss @ time 214: 9.552678108215332\n",
            "Loss @ time 215: 9.512825965881348\n",
            "Loss @ time 216: 9.472759246826172\n",
            "Loss @ time 217: 9.432586669921875\n",
            "Loss @ time 218: 9.392167091369629\n",
            "Loss @ time 219: 9.35128402709961\n",
            "Loss @ time 220: 9.310110092163086\n",
            "Loss @ time 221: 9.268948554992676\n",
            "Loss @ time 222: 9.227508544921875\n",
            "Loss @ time 223: 9.18560791015625\n",
            "Loss @ time 224: 9.143548965454102\n",
            "Loss @ time 225: 9.101419448852539\n",
            "Loss @ time 226: 9.059057235717773\n",
            "Loss @ time 227: 9.01638412475586\n",
            "Loss @ time 228: 8.973616600036621\n",
            "Loss @ time 229: 8.93058967590332\n",
            "Loss @ time 230: 8.88743782043457\n",
            "Loss @ time 231: 8.844200134277344\n",
            "Loss @ time 232: 8.800878524780273\n",
            "Loss @ time 233: 8.757678031921387\n",
            "Loss @ time 234: 8.714424133300781\n",
            "Loss @ time 235: 8.671154022216797\n",
            "Loss @ time 236: 8.627795219421387\n",
            "Loss @ time 237: 8.584305763244629\n",
            "Loss @ time 238: 8.540602684020996\n",
            "Loss @ time 239: 8.496675491333008\n",
            "Loss @ time 240: 8.452664375305176\n",
            "Loss @ time 241: 8.408559799194336\n",
            "Loss @ time 242: 8.364243507385254\n",
            "Loss @ time 243: 8.319867134094238\n",
            "Loss @ time 244: 8.275130271911621\n",
            "Loss @ time 245: 8.230352401733398\n",
            "Loss @ time 246: 8.185725212097168\n",
            "Loss @ time 247: 8.141188621520996\n",
            "Loss @ time 248: 8.096500396728516\n",
            "Loss @ time 249: 8.051732063293457\n",
            "Loss @ time 250: 8.006799697875977\n",
            "Loss @ time 251: 7.961771011352539\n",
            "Loss @ time 252: 7.9166765213012695\n",
            "Loss @ time 253: 7.871352672576904\n",
            "Loss @ time 254: 7.825841903686523\n",
            "Loss @ time 255: 7.780345916748047\n",
            "Loss @ time 256: 7.734330654144287\n",
            "Loss @ time 257: 7.688374042510986\n",
            "Loss @ time 258: 7.642158031463623\n",
            "Loss @ time 259: 7.595721244812012\n",
            "Loss @ time 260: 7.5492048263549805\n",
            "Loss @ time 261: 7.502747058868408\n",
            "Loss @ time 262: 7.455817222595215\n",
            "Loss @ time 263: 7.409041881561279\n",
            "Loss @ time 264: 7.361873626708984\n",
            "Loss @ time 265: 7.3147711753845215\n",
            "Loss @ time 266: 7.267785549163818\n",
            "Loss @ time 267: 7.220712661743164\n",
            "Loss @ time 268: 7.173193454742432\n",
            "Loss @ time 269: 7.125614643096924\n",
            "Loss @ time 270: 7.078207015991211\n",
            "Loss @ time 271: 7.03041934967041\n",
            "Loss @ time 272: 6.982625961303711\n",
            "Loss @ time 273: 6.9349470138549805\n",
            "Loss @ time 274: 6.887577533721924\n",
            "Loss @ time 275: 6.840141773223877\n",
            "Loss @ time 276: 6.7929277420043945\n",
            "Loss @ time 277: 6.745914459228516\n",
            "Loss @ time 278: 6.699106216430664\n",
            "Loss @ time 279: 6.651954174041748\n",
            "Loss @ time 280: 6.605266571044922\n",
            "Loss @ time 281: 6.558362007141113\n",
            "Loss @ time 282: 6.511373996734619\n",
            "Loss @ time 283: 6.464987754821777\n",
            "Loss @ time 284: 6.417972564697266\n",
            "Loss @ time 285: 6.370532035827637\n",
            "Loss @ time 286: 6.323490619659424\n",
            "Loss @ time 287: 6.2775115966796875\n",
            "Loss @ time 288: 6.2306318283081055\n",
            "Loss @ time 289: 6.183331489562988\n",
            "Loss @ time 290: 6.136818885803223\n",
            "Loss @ time 291: 6.0889105796813965\n",
            "Loss @ time 292: 6.042058944702148\n",
            "Loss @ time 293: 5.995433807373047\n",
            "Loss @ time 294: 5.948637008666992\n",
            "Loss @ time 295: 5.901973724365234\n",
            "Loss @ time 296: 5.855733871459961\n",
            "Loss @ time 297: 5.809232234954834\n",
            "Loss @ time 298: 5.762363910675049\n",
            "Loss @ time 299: 5.714879035949707\n",
            "Loss @ time 300: 5.667275905609131\n",
            "Loss @ time 301: 5.619796276092529\n",
            "Loss @ time 302: 5.572403430938721\n",
            "Loss @ time 303: 5.52457332611084\n",
            "Loss @ time 304: 5.477366924285889\n",
            "Loss @ time 305: 5.429914474487305\n",
            "Loss @ time 306: 5.382917881011963\n",
            "Loss @ time 307: 5.335525989532471\n",
            "Loss @ time 308: 5.287887096405029\n",
            "Loss @ time 309: 5.240650653839111\n",
            "Loss @ time 310: 5.190832614898682\n",
            "Loss @ time 311: 5.139707565307617\n",
            "Loss @ time 312: 5.092293739318848\n",
            "Loss @ time 313: 5.049096584320068\n",
            "Loss @ time 314: 5.002832412719727\n",
            "Loss @ time 315: 4.9526472091674805\n",
            "Loss @ time 316: 4.90343713760376\n",
            "Loss @ time 317: 4.857528209686279\n",
            "Loss @ time 318: 4.81265115737915\n",
            "Loss @ time 319: 4.766068935394287\n",
            "Loss @ time 320: 4.7184858322143555\n",
            "Loss @ time 321: 4.670419216156006\n",
            "Loss @ time 322: 4.623575687408447\n",
            "Loss @ time 323: 4.5775146484375\n",
            "Loss @ time 324: 4.531346321105957\n",
            "Loss @ time 325: 4.48418664932251\n",
            "Loss @ time 326: 4.43583869934082\n",
            "Loss @ time 327: 4.38645076751709\n",
            "Loss @ time 328: 4.337344646453857\n",
            "Loss @ time 329: 4.291924953460693\n",
            "Loss @ time 330: 4.248798370361328\n",
            "Loss @ time 331: 4.2038254737854\n",
            "Loss @ time 332: 4.1573076248168945\n",
            "Loss @ time 333: 4.110721111297607\n",
            "Loss @ time 334: 4.065624237060547\n",
            "Loss @ time 335: 4.020570755004883\n",
            "Loss @ time 336: 3.9755077362060547\n",
            "Loss @ time 337: 3.9294211864471436\n",
            "Loss @ time 338: 3.880902051925659\n",
            "Loss @ time 339: 3.8286168575286865\n",
            "Loss @ time 340: 3.773653030395508\n",
            "Loss @ time 341: 3.717435598373413\n",
            "Loss @ time 342: 3.6660308837890625\n",
            "Loss @ time 343: 3.624239206314087\n",
            "Loss @ time 344: 3.589627981185913\n",
            "Loss @ time 345: 3.5500283241271973\n",
            "Loss @ time 346: 3.505669593811035\n",
            "Loss @ time 347: 3.4545650482177734\n",
            "Loss @ time 348: 3.4068737030029297\n",
            "Loss @ time 349: 3.3632843494415283\n",
            "Loss @ time 350: 3.321276903152466\n",
            "Loss @ time 351: 3.2807369232177734\n",
            "Loss @ time 352: 3.2405965328216553\n",
            "Loss @ time 353: 3.1999623775482178\n",
            "Loss @ time 354: 3.158170223236084\n",
            "Loss @ time 355: 3.115628719329834\n",
            "Loss @ time 356: 3.073981523513794\n",
            "Loss @ time 357: 3.0340492725372314\n",
            "Loss @ time 358: 2.9950530529022217\n",
            "Loss @ time 359: 2.957151174545288\n",
            "Loss @ time 360: 2.9206693172454834\n",
            "Loss @ time 361: 2.886871099472046\n",
            "Loss @ time 362: 2.853510856628418\n",
            "Loss @ time 363: 2.815312385559082\n",
            "Loss @ time 364: 2.769895315170288\n",
            "Loss @ time 365: 2.7242801189422607\n",
            "Loss @ time 366: 2.688338279724121\n",
            "Loss @ time 367: 2.658470392227173\n",
            "Loss @ time 368: 2.624528408050537\n",
            "Loss @ time 369: 2.583287239074707\n",
            "Loss @ time 370: 2.5440030097961426\n",
            "Loss @ time 371: 2.510925531387329\n",
            "Loss @ time 372: 2.4784481525421143\n",
            "Loss @ time 373: 2.4423952102661133\n",
            "Loss @ time 374: 2.4068827629089355\n",
            "Loss @ time 375: 2.375945806503296\n",
            "Loss @ time 376: 2.3466391563415527\n",
            "Loss @ time 377: 2.314633369445801\n",
            "Loss @ time 378: 2.2802109718322754\n",
            "Loss @ time 379: 2.247800827026367\n",
            "Loss @ time 380: 2.2191221714019775\n",
            "Loss @ time 381: 2.190432071685791\n",
            "Loss @ time 382: 2.1606037616729736\n",
            "Loss @ time 383: 2.1309196949005127\n",
            "Loss @ time 384: 2.10237717628479\n",
            "Loss @ time 385: 2.074852705001831\n",
            "Loss @ time 386: 2.0471577644348145\n",
            "Loss @ time 387: 2.01895809173584\n",
            "Loss @ time 388: 1.9910544157028198\n",
            "Loss @ time 389: 1.9640368223190308\n",
            "Loss @ time 390: 1.9378925561904907\n",
            "Loss @ time 391: 1.911779761314392\n",
            "Loss @ time 392: 1.88503098487854\n",
            "Loss @ time 393: 1.8590893745422363\n",
            "Loss @ time 394: 1.8343381881713867\n",
            "Loss @ time 395: 1.8105711936950684\n",
            "Loss @ time 396: 1.7872980833053589\n",
            "Loss @ time 397: 1.7640423774719238\n",
            "Loss @ time 398: 1.7406606674194336\n",
            "Loss @ time 399: 1.7171850204467773\n",
            "Loss @ time 400: 1.6940346956253052\n",
            "Loss @ time 401: 1.6716846227645874\n",
            "Loss @ time 402: 1.6498289108276367\n",
            "Loss @ time 403: 1.6286784410476685\n",
            "Loss @ time 404: 1.6079745292663574\n",
            "Loss @ time 405: 1.5875344276428223\n",
            "Loss @ time 406: 1.5672383308410645\n",
            "Loss @ time 407: 1.5472944974899292\n",
            "Loss @ time 408: 1.5276718139648438\n",
            "Loss @ time 409: 1.5084630250930786\n",
            "Loss @ time 410: 1.4897445440292358\n",
            "Loss @ time 411: 1.4714688062667847\n",
            "Loss @ time 412: 1.4535342454910278\n",
            "Loss @ time 413: 1.4360159635543823\n",
            "Loss @ time 414: 1.4189077615737915\n",
            "Loss @ time 415: 1.4022042751312256\n",
            "Loss @ time 416: 1.3859347105026245\n",
            "Loss @ time 417: 1.3701963424682617\n",
            "Loss @ time 418: 1.3551650047302246\n",
            "Loss @ time 419: 1.3409651517868042\n",
            "Loss @ time 420: 1.3279335498809814\n",
            "Loss @ time 421: 1.316252589225769\n",
            "Loss @ time 422: 1.3057206869125366\n",
            "Loss @ time 423: 1.294835090637207\n",
            "Loss @ time 424: 1.2807824611663818\n",
            "Loss @ time 425: 1.2621891498565674\n",
            "Loss @ time 426: 1.2420316934585571\n",
            "Loss @ time 427: 1.2258493900299072\n",
            "Loss @ time 428: 1.2152043581008911\n",
            "Loss @ time 429: 1.2067835330963135\n",
            "Loss @ time 430: 1.1955828666687012\n",
            "Loss @ time 431: 1.1801851987838745\n",
            "Loss @ time 432: 1.1638765335083008\n",
            "Loss @ time 433: 1.1503826379776\n",
            "Loss @ time 434: 1.1401172876358032\n",
            "Loss @ time 435: 1.130527138710022\n",
            "Loss @ time 436: 1.1187714338302612\n",
            "Loss @ time 437: 1.1055536270141602\n",
            "Loss @ time 438: 1.093570351600647\n",
            "Loss @ time 439: 1.083765983581543\n",
            "Loss @ time 440: 1.0757025480270386\n",
            "Loss @ time 441: 1.066882610321045\n",
            "Loss @ time 442: 1.0567635297775269\n",
            "Loss @ time 443: 1.0468072891235352\n",
            "Loss @ time 444: 1.03789222240448\n",
            "Loss @ time 445: 1.0298752784729004\n",
            "Loss @ time 446: 1.02199387550354\n",
            "Loss @ time 447: 1.0136945247650146\n",
            "Loss @ time 448: 1.0049290657043457\n",
            "Loss @ time 449: 0.9962834119796753\n",
            "Loss @ time 450: 0.9882957935333252\n",
            "Loss @ time 451: 0.9807206988334656\n",
            "Loss @ time 452: 0.9733685255050659\n",
            "Loss @ time 453: 0.9659702181816101\n",
            "Loss @ time 454: 0.9585624933242798\n",
            "Loss @ time 455: 0.9512326121330261\n",
            "Loss @ time 456: 0.9439828991889954\n",
            "Loss @ time 457: 0.9372026920318604\n",
            "Loss @ time 458: 0.9305067658424377\n",
            "Loss @ time 459: 0.923862636089325\n",
            "Loss @ time 460: 0.9174591302871704\n",
            "Loss @ time 461: 0.9111553430557251\n",
            "Loss @ time 462: 0.9049583673477173\n",
            "Loss @ time 463: 0.8987773060798645\n",
            "Loss @ time 464: 0.892723023891449\n",
            "Loss @ time 465: 0.8868948221206665\n",
            "Loss @ time 466: 0.8809874057769775\n",
            "Loss @ time 467: 0.8751882314682007\n",
            "Loss @ time 468: 0.8694327473640442\n",
            "Loss @ time 469: 0.8635505437850952\n",
            "Loss @ time 470: 0.857707679271698\n",
            "Loss @ time 471: 0.8521494269371033\n",
            "Loss @ time 472: 0.8469339609146118\n",
            "Loss @ time 473: 0.841828465461731\n",
            "Loss @ time 474: 0.8369563221931458\n",
            "Loss @ time 475: 0.8323380947113037\n",
            "Loss @ time 476: 0.828043520450592\n",
            "Loss @ time 477: 0.8242812156677246\n",
            "Loss @ time 478: 0.821290910243988\n",
            "Loss @ time 479: 0.8195247054100037\n",
            "Loss @ time 480: 0.8187048435211182\n",
            "Loss @ time 481: 0.8182323575019836\n",
            "Loss @ time 482: 0.8163091540336609\n",
            "Loss @ time 483: 0.8105973601341248\n",
            "Loss @ time 484: 0.8009786009788513\n",
            "Loss @ time 485: 0.7899385094642639\n",
            "Loss @ time 486: 0.7812892198562622\n",
            "Loss @ time 487: 0.7768658399581909\n",
            "Loss @ time 488: 0.77524733543396\n",
            "Loss @ time 489: 0.7740809917449951\n",
            "Loss @ time 490: 0.7712377905845642\n",
            "Loss @ time 491: 0.7666347026824951\n",
            "Loss @ time 492: 0.7607813477516174\n",
            "Loss @ time 493: 0.7548190951347351\n",
            "Loss @ time 494: 0.7497907876968384\n",
            "Loss @ time 495: 0.746310293674469\n",
            "Loss @ time 496: 0.7439460754394531\n",
            "Loss @ time 497: 0.7416498064994812\n",
            "Loss @ time 498: 0.7387453317642212\n",
            "Loss @ time 499: 0.7349626421928406\n",
            "Loss @ time 500: 0.7304148077964783\n",
            "Loss @ time 501: 0.7260090708732605\n",
            "Loss @ time 502: 0.7225404977798462\n",
            "Loss @ time 503: 0.719950795173645\n",
            "Loss @ time 504: 0.7174810171127319\n",
            "Loss @ time 505: 0.7149704694747925\n",
            "Loss @ time 506: 0.7116497159004211\n",
            "Loss @ time 507: 0.7076866626739502\n",
            "Loss @ time 508: 0.7029299139976501\n",
            "Loss @ time 509: 0.6983722448348999\n",
            "Loss @ time 510: 0.694050133228302\n",
            "Loss @ time 511: 0.689861536026001\n",
            "Loss @ time 512: 0.6859474182128906\n",
            "Loss @ time 513: 0.6825278997421265\n",
            "Loss @ time 514: 0.6787312030792236\n",
            "Loss @ time 515: 0.6744415163993835\n",
            "Loss @ time 516: 0.6700976490974426\n",
            "Loss @ time 517: 0.6663225293159485\n",
            "Loss @ time 518: 0.6628409028053284\n",
            "Loss @ time 519: 0.6599265336990356\n",
            "Loss @ time 520: 0.6571625471115112\n",
            "Loss @ time 521: 0.6547565460205078\n",
            "Loss @ time 522: 0.652632474899292\n",
            "Loss @ time 523: 0.6504471302032471\n",
            "Loss @ time 524: 0.6484361886978149\n",
            "Loss @ time 525: 0.6464284658432007\n",
            "Loss @ time 526: 0.6443337798118591\n",
            "Loss @ time 527: 0.6423747539520264\n",
            "Loss @ time 528: 0.6406762003898621\n",
            "Loss @ time 529: 0.6393443942070007\n",
            "Loss @ time 530: 0.6379776000976562\n",
            "Loss @ time 531: 0.6367123126983643\n",
            "Loss @ time 532: 0.6348296403884888\n",
            "Loss @ time 533: 0.6324770450592041\n",
            "Loss @ time 534: 0.6296328902244568\n",
            "Loss @ time 535: 0.6264886260032654\n",
            "Loss @ time 536: 0.6229686141014099\n",
            "Loss @ time 537: 0.6188172698020935\n",
            "Loss @ time 538: 0.614412248134613\n",
            "Loss @ time 539: 0.6104815006256104\n",
            "Loss @ time 540: 0.6075648069381714\n",
            "Loss @ time 541: 0.605935275554657\n",
            "Loss @ time 542: 0.6050462126731873\n",
            "Loss @ time 543: 0.6041617393493652\n",
            "Loss @ time 544: 0.6025102734565735\n",
            "Loss @ time 545: 0.5997825264930725\n",
            "Loss @ time 546: 0.5964400172233582\n",
            "Loss @ time 547: 0.5931286811828613\n",
            "Loss @ time 548: 0.5900645852088928\n",
            "Loss @ time 549: 0.5876442193984985\n",
            "Loss @ time 550: 0.5859591960906982\n",
            "Loss @ time 551: 0.5846928358078003\n",
            "Loss @ time 552: 0.5836149454116821\n",
            "Loss @ time 553: 0.5823644399642944\n",
            "Loss @ time 554: 0.5810470581054688\n",
            "Loss @ time 555: 0.5796896815299988\n",
            "Loss @ time 556: 0.5781146883964539\n",
            "Loss @ time 557: 0.5763914585113525\n",
            "Loss @ time 558: 0.5748894214630127\n",
            "Loss @ time 559: 0.5735373497009277\n",
            "Loss @ time 560: 0.5726344585418701\n",
            "Loss @ time 561: 0.5717657208442688\n",
            "Loss @ time 562: 0.5706214308738708\n",
            "Loss @ time 563: 0.5686317682266235\n",
            "Loss @ time 564: 0.5659903883934021\n",
            "Loss @ time 565: 0.5625868439674377\n",
            "Loss @ time 566: 0.5591301918029785\n",
            "Loss @ time 567: 0.5562512874603271\n",
            "Loss @ time 568: 0.554327666759491\n",
            "Loss @ time 569: 0.5532937049865723\n",
            "Loss @ time 570: 0.5527842044830322\n",
            "Loss @ time 571: 0.5525264143943787\n",
            "Loss @ time 572: 0.5519253611564636\n",
            "Loss @ time 573: 0.5505118370056152\n",
            "Loss @ time 574: 0.5484753251075745\n",
            "Loss @ time 575: 0.5455219149589539\n",
            "Loss @ time 576: 0.5420591235160828\n",
            "Loss @ time 577: 0.5385946035385132\n",
            "Loss @ time 578: 0.5355287790298462\n",
            "Loss @ time 579: 0.5332289338111877\n",
            "Loss @ time 580: 0.531514048576355\n",
            "Loss @ time 581: 0.5301955342292786\n",
            "Loss @ time 582: 0.5291100144386292\n",
            "Loss @ time 583: 0.5280650854110718\n",
            "Loss @ time 584: 0.5270427465438843\n",
            "Loss @ time 585: 0.5260878801345825\n",
            "Loss @ time 586: 0.5249325633049011\n",
            "Loss @ time 587: 0.5238365530967712\n",
            "Loss @ time 588: 0.5223134756088257\n",
            "Loss @ time 589: 0.5206493735313416\n",
            "Loss @ time 590: 0.5190048217773438\n",
            "Loss @ time 591: 0.5174781680107117\n",
            "Loss @ time 592: 0.5164169669151306\n",
            "Loss @ time 593: 0.51609206199646\n",
            "Loss @ time 594: 0.5169510841369629\n",
            "Loss @ time 595: 0.5195723176002502\n",
            "Loss @ time 596: 0.5244134664535522\n",
            "Loss @ time 597: 0.5305200815200806\n",
            "Loss @ time 598: 0.5342807173728943\n",
            "Loss @ time 599: 0.5302718281745911\n",
            "Loss @ time 600: 0.5173829793930054\n",
            "Loss @ time 601: 0.5043370127677917\n",
            "Loss @ time 602: 0.5009561777114868\n",
            "Loss @ time 603: 0.5065177083015442\n",
            "Loss @ time 604: 0.5116588473320007\n",
            "Loss @ time 605: 0.5086300373077393\n",
            "Loss @ time 606: 0.500244140625\n",
            "Loss @ time 607: 0.49560627341270447\n",
            "Loss @ time 608: 0.4983704686164856\n",
            "Loss @ time 609: 0.5028951168060303\n",
            "Loss @ time 610: 0.502814531326294\n",
            "Loss @ time 611: 0.499103844165802\n",
            "Loss @ time 612: 0.49687376618385315\n",
            "Loss @ time 613: 0.4985811710357666\n",
            "Loss @ time 614: 0.5012874007225037\n",
            "Loss @ time 615: 0.4991390109062195\n",
            "Loss @ time 616: 0.4922730326652527\n",
            "Loss @ time 617: 0.485530823469162\n",
            "Loss @ time 618: 0.4828609824180603\n",
            "Loss @ time 619: 0.48312586545944214\n",
            "Loss @ time 620: 0.4831105172634125\n",
            "Loss @ time 621: 0.48228785395622253\n",
            "Loss @ time 622: 0.4818352460861206\n",
            "Loss @ time 623: 0.48228177428245544\n",
            "Loss @ time 624: 0.4819781482219696\n",
            "Loss @ time 625: 0.47955840826034546\n",
            "Loss @ time 626: 0.4756697118282318\n",
            "Loss @ time 627: 0.47262537479400635\n",
            "Loss @ time 628: 0.4714583158493042\n",
            "Loss @ time 629: 0.4714798033237457\n",
            "Loss @ time 630: 0.4715113043785095\n",
            "Loss @ time 631: 0.47089260816574097\n",
            "Loss @ time 632: 0.47016364336013794\n",
            "Loss @ time 633: 0.4694894850254059\n",
            "Loss @ time 634: 0.46873965859413147\n",
            "Loss @ time 635: 0.4673599898815155\n",
            "Loss @ time 636: 0.4653956890106201\n",
            "Loss @ time 637: 0.46347296237945557\n",
            "Loss @ time 638: 0.4622284173965454\n",
            "Loss @ time 639: 0.46166011691093445\n",
            "Loss @ time 640: 0.4613531231880188\n",
            "Loss @ time 641: 0.4609489440917969\n",
            "Loss @ time 642: 0.460464745759964\n",
            "Loss @ time 643: 0.4600093364715576\n",
            "Loss @ time 644: 0.45961737632751465\n",
            "Loss @ time 645: 0.45923367142677307\n",
            "Loss @ time 646: 0.45868831872940063\n",
            "Loss @ time 647: 0.45787671208381653\n",
            "Loss @ time 648: 0.4567238986492157\n",
            "Loss @ time 649: 0.45540595054626465\n",
            "Loss @ time 650: 0.4541790783405304\n",
            "Loss @ time 651: 0.4530280828475952\n",
            "Loss @ time 652: 0.4519573152065277\n",
            "Loss @ time 653: 0.45104193687438965\n",
            "Loss @ time 654: 0.45012760162353516\n",
            "Loss @ time 655: 0.44935986399650574\n",
            "Loss @ time 656: 0.44859984517097473\n",
            "Loss @ time 657: 0.4479232430458069\n",
            "Loss @ time 658: 0.4473223090171814\n",
            "Loss @ time 659: 0.44679173827171326\n",
            "Loss @ time 660: 0.4463890790939331\n",
            "Loss @ time 661: 0.44613975286483765\n",
            "Loss @ time 662: 0.44610458612442017\n",
            "Loss @ time 663: 0.4463357627391815\n",
            "Loss @ time 664: 0.4469841718673706\n",
            "Loss @ time 665: 0.4479907751083374\n",
            "Loss @ time 666: 0.4494927227497101\n",
            "Loss @ time 667: 0.4510970413684845\n",
            "Loss @ time 668: 0.4527968466281891\n",
            "Loss @ time 669: 0.4531027674674988\n",
            "Loss @ time 670: 0.45165935158729553\n",
            "Loss @ time 671: 0.44799819588661194\n",
            "Loss @ time 672: 0.4432317316532135\n",
            "Loss @ time 673: 0.4390413165092468\n",
            "Loss @ time 674: 0.43676063418388367\n",
            "Loss @ time 675: 0.4368330240249634\n",
            "Loss @ time 676: 0.4383822977542877\n",
            "Loss @ time 677: 0.4399813711643219\n",
            "Loss @ time 678: 0.44039490818977356\n",
            "Loss @ time 679: 0.44125470519065857\n",
            "Loss @ time 680: 0.44303879141807556\n",
            "Loss @ time 681: 0.4442886412143707\n",
            "Loss @ time 682: 0.44354355335235596\n",
            "Loss @ time 683: 0.4408540427684784\n",
            "Loss @ time 684: 0.4365776777267456\n",
            "Loss @ time 685: 0.43238890171051025\n",
            "Loss @ time 686: 0.4301813244819641\n",
            "Loss @ time 687: 0.4301730990409851\n",
            "Loss @ time 688: 0.4315771758556366\n",
            "Loss @ time 689: 0.43314483761787415\n",
            "Loss @ time 690: 0.43386611342430115\n",
            "Loss @ time 691: 0.4332209825515747\n",
            "Loss @ time 692: 0.4315847158432007\n",
            "Loss @ time 693: 0.4299546778202057\n",
            "Loss @ time 694: 0.4296031892299652\n",
            "Loss @ time 695: 0.4303421676158905\n",
            "Loss @ time 696: 0.431789755821228\n",
            "Loss @ time 697: 0.433201402425766\n",
            "Loss @ time 698: 0.4331595301628113\n",
            "Loss @ time 699: 0.43123698234558105\n",
            "Loss @ time 700: 0.42860671877861023\n",
            "Loss @ time 701: 0.4258902370929718\n",
            "Loss @ time 702: 0.4234437048435211\n",
            "Loss @ time 703: 0.42201828956604004\n",
            "Loss @ time 704: 0.4212615191936493\n",
            "Loss @ time 705: 0.4215967655181885\n",
            "Loss @ time 706: 0.42194807529449463\n",
            "Loss @ time 707: 0.42194071412086487\n",
            "Loss @ time 708: 0.42198505997657776\n",
            "Loss @ time 709: 0.42165932059288025\n",
            "Loss @ time 710: 0.4215211868286133\n",
            "Loss @ time 711: 0.42087966203689575\n",
            "Loss @ time 712: 0.4199592173099518\n",
            "Loss @ time 713: 0.41917315125465393\n",
            "Loss @ time 714: 0.41837769746780396\n",
            "Loss @ time 715: 0.4176228642463684\n",
            "Loss @ time 716: 0.4170367419719696\n",
            "Loss @ time 717: 0.4164700210094452\n",
            "Loss @ time 718: 0.416034460067749\n",
            "Loss @ time 719: 0.4155784845352173\n",
            "Loss @ time 720: 0.41510772705078125\n",
            "Loss @ time 721: 0.4147263765335083\n",
            "Loss @ time 722: 0.41438761353492737\n",
            "Loss @ time 723: 0.4139843285083771\n",
            "Loss @ time 724: 0.41373366117477417\n",
            "Loss @ time 725: 0.41337084770202637\n",
            "Loss @ time 726: 0.4131157696247101\n",
            "Loss @ time 727: 0.4129961431026459\n",
            "Loss @ time 728: 0.41310855746269226\n",
            "Loss @ time 729: 0.41322338581085205\n",
            "Loss @ time 730: 0.4135489761829376\n",
            "Loss @ time 731: 0.4142921566963196\n",
            "Loss @ time 732: 0.41492655873298645\n",
            "Loss @ time 733: 0.4152010679244995\n",
            "Loss @ time 734: 0.41574516892433167\n",
            "Loss @ time 735: 0.4160524308681488\n",
            "Loss @ time 736: 0.4159301519393921\n",
            "Loss @ time 737: 0.41585394740104675\n",
            "Loss @ time 738: 0.41569727659225464\n",
            "Loss @ time 739: 0.41554051637649536\n",
            "Loss @ time 740: 0.41514071822166443\n",
            "Loss @ time 741: 0.4152563810348511\n",
            "Loss @ time 742: 0.41546016931533813\n",
            "Loss @ time 743: 0.4149802327156067\n",
            "Loss @ time 744: 0.41353166103363037\n",
            "Loss @ time 745: 0.4118976891040802\n",
            "Loss @ time 746: 0.40984824299812317\n",
            "Loss @ time 747: 0.4083766043186188\n",
            "Loss @ time 748: 0.4075857102870941\n",
            "Loss @ time 749: 0.4071168899536133\n",
            "Loss @ time 750: 0.4070172607898712\n",
            "Loss @ time 751: 0.4074802100658417\n",
            "Loss @ time 752: 0.40792787075042725\n",
            "Loss @ time 753: 0.40811455249786377\n",
            "Loss @ time 754: 0.40774214267730713\n",
            "Loss @ time 755: 0.40703722834587097\n",
            "Loss @ time 756: 0.4061391353607178\n",
            "Loss @ time 757: 0.40482592582702637\n",
            "Loss @ time 758: 0.4038417935371399\n",
            "Loss @ time 759: 0.4029877781867981\n",
            "Loss @ time 760: 0.40247729420661926\n",
            "Loss @ time 761: 0.4026893675327301\n",
            "Loss @ time 762: 0.40349364280700684\n",
            "Loss @ time 763: 0.4046666622161865\n",
            "Loss @ time 764: 0.40548279881477356\n",
            "Loss @ time 765: 0.4057374596595764\n",
            "Loss @ time 766: 0.4054558277130127\n",
            "Loss @ time 767: 0.40453362464904785\n",
            "Loss @ time 768: 0.4036496877670288\n",
            "Loss @ time 769: 0.4022500216960907\n",
            "Loss @ time 770: 0.40073543787002563\n",
            "Loss @ time 771: 0.39952096343040466\n",
            "Loss @ time 772: 0.3990621268749237\n",
            "Loss @ time 773: 0.39934372901916504\n",
            "Loss @ time 774: 0.3997706174850464\n",
            "Loss @ time 775: 0.40008649230003357\n",
            "Loss @ time 776: 0.40004032850265503\n",
            "Loss @ time 777: 0.3997975289821625\n",
            "Loss @ time 778: 0.39959943294525146\n",
            "Loss @ time 779: 0.3991936445236206\n",
            "Loss @ time 780: 0.3988558053970337\n",
            "Loss @ time 781: 0.3980640769004822\n",
            "Loss @ time 782: 0.3969103991985321\n",
            "Loss @ time 783: 0.3958136737346649\n",
            "Loss @ time 784: 0.39488106966018677\n",
            "Loss @ time 785: 0.39405590295791626\n",
            "Loss @ time 786: 0.39368489384651184\n",
            "Loss @ time 787: 0.39333075284957886\n",
            "Loss @ time 788: 0.39311355352401733\n",
            "Loss @ time 789: 0.3930583596229553\n",
            "Loss @ time 790: 0.3929409384727478\n",
            "Loss @ time 791: 0.3932654857635498\n",
            "Loss @ time 792: 0.3934474289417267\n",
            "Loss @ time 793: 0.39390918612480164\n",
            "Loss @ time 794: 0.39416760206222534\n",
            "Loss @ time 795: 0.39489927887916565\n",
            "Loss @ time 796: 0.3960111737251282\n",
            "Loss @ time 797: 0.39733994007110596\n",
            "Loss @ time 798: 0.398665189743042\n",
            "Loss @ time 799: 0.40019404888153076\n",
            "Loss @ time 800: 0.40097805857658386\n",
            "Loss @ time 801: 0.4008125066757202\n",
            "Loss @ time 802: 0.39945361018180847\n",
            "Loss @ time 803: 0.3970596194267273\n",
            "Loss @ time 804: 0.39370858669281006\n",
            "Loss @ time 805: 0.39070358872413635\n",
            "Loss @ time 806: 0.3888009190559387\n",
            "Loss @ time 807: 0.38837680220603943\n",
            "Loss @ time 808: 0.3893420696258545\n",
            "Loss @ time 809: 0.39094260334968567\n",
            "Loss @ time 810: 0.3925962448120117\n",
            "Loss @ time 811: 0.3938698172569275\n",
            "Loss @ time 812: 0.3946775496006012\n",
            "Loss @ time 813: 0.3948322534561157\n",
            "Loss @ time 814: 0.39448779821395874\n",
            "Loss @ time 815: 0.3943803906440735\n",
            "Loss @ time 816: 0.39449062943458557\n",
            "Loss @ time 817: 0.3956686854362488\n",
            "Loss @ time 818: 0.3985500633716583\n",
            "Loss @ time 819: 0.4008611738681793\n",
            "Loss @ time 820: 0.401228666305542\n",
            "Loss @ time 821: 0.3993472754955292\n",
            "Loss @ time 822: 0.39562347531318665\n",
            "Loss @ time 823: 0.3910864293575287\n",
            "Loss @ time 824: 0.38741523027420044\n",
            "Loss @ time 825: 0.3862771689891815\n",
            "Loss @ time 826: 0.3872236907482147\n",
            "Loss @ time 827: 0.38893526792526245\n",
            "Loss @ time 828: 0.39000189304351807\n",
            "Loss @ time 829: 0.3892199397087097\n",
            "Loss @ time 830: 0.3870432674884796\n",
            "Loss @ time 831: 0.3847898542881012\n",
            "Loss @ time 832: 0.3836124837398529\n",
            "Loss @ time 833: 0.38347718119621277\n",
            "Loss @ time 834: 0.3842536211013794\n",
            "Loss @ time 835: 0.3849303722381592\n",
            "Loss @ time 836: 0.3850148022174835\n",
            "Loss @ time 837: 0.38452935218811035\n",
            "Loss @ time 838: 0.3833301067352295\n",
            "Loss @ time 839: 0.3819980323314667\n",
            "Loss @ time 840: 0.380656361579895\n",
            "Loss @ time 841: 0.3796444833278656\n",
            "Loss @ time 842: 0.37895670533180237\n",
            "Loss @ time 843: 0.37848353385925293\n",
            "Loss @ time 844: 0.37824946641921997\n",
            "Loss @ time 845: 0.37808242440223694\n",
            "Loss @ time 846: 0.3781827986240387\n",
            "Loss @ time 847: 0.3781600296497345\n",
            "Loss @ time 848: 0.3780735731124878\n",
            "Loss @ time 849: 0.37782713770866394\n",
            "Loss @ time 850: 0.37776410579681396\n",
            "Loss @ time 851: 0.3775407671928406\n",
            "Loss @ time 852: 0.377507746219635\n",
            "Loss @ time 853: 0.37741944193840027\n",
            "Loss @ time 854: 0.37749233841896057\n",
            "Loss @ time 855: 0.3778017461299896\n",
            "Loss @ time 856: 0.37808340787887573\n",
            "Loss @ time 857: 0.3784862458705902\n",
            "Loss @ time 858: 0.3788372278213501\n",
            "Loss @ time 859: 0.37936699390411377\n",
            "Loss @ time 860: 0.3796336054801941\n",
            "Loss @ time 861: 0.3796468675136566\n",
            "Loss @ time 862: 0.37954115867614746\n",
            "Loss @ time 863: 0.3793284595012665\n",
            "Loss @ time 864: 0.3785002529621124\n",
            "Loss @ time 865: 0.3772747814655304\n",
            "Loss @ time 866: 0.37599122524261475\n",
            "Loss @ time 867: 0.3749285936355591\n",
            "Loss @ time 868: 0.3738834261894226\n",
            "Loss @ time 869: 0.3730722665786743\n",
            "Loss @ time 870: 0.37237560749053955\n",
            "Loss @ time 871: 0.3720253109931946\n",
            "Loss @ time 872: 0.3723422586917877\n",
            "Loss @ time 873: 0.3728731870651245\n",
            "Loss @ time 874: 0.3733323812484741\n",
            "Loss @ time 875: 0.37359827756881714\n",
            "Loss @ time 876: 0.37385162711143494\n",
            "Loss @ time 877: 0.37405532598495483\n",
            "Loss @ time 878: 0.3744852542877197\n",
            "Loss @ time 879: 0.3743925988674164\n",
            "Loss @ time 880: 0.37416037917137146\n",
            "Loss @ time 881: 0.37380462884902954\n",
            "Loss @ time 882: 0.3734281063079834\n",
            "Loss @ time 883: 0.37285667657852173\n",
            "Loss @ time 884: 0.37186726927757263\n",
            "Loss @ time 885: 0.3710264265537262\n",
            "Loss @ time 886: 0.370349645614624\n",
            "Loss @ time 887: 0.369719922542572\n",
            "Loss @ time 888: 0.36901816725730896\n",
            "Loss @ time 889: 0.3687979578971863\n",
            "Loss @ time 890: 0.36876338720321655\n",
            "Loss @ time 891: 0.36864808201789856\n",
            "Loss @ time 892: 0.3687078058719635\n",
            "Loss @ time 893: 0.3689744174480438\n",
            "Loss @ time 894: 0.36909377574920654\n",
            "Loss @ time 895: 0.3693005442619324\n",
            "Loss @ time 896: 0.36963707208633423\n",
            "Loss @ time 897: 0.3701029121875763\n",
            "Loss @ time 898: 0.3712354302406311\n",
            "Loss @ time 899: 0.37264060974121094\n",
            "Loss @ time 900: 0.3743837773799896\n",
            "Loss @ time 901: 0.37678390741348267\n",
            "Loss @ time 902: 0.37962380051612854\n",
            "Loss @ time 903: 0.38268956542015076\n",
            "Loss @ time 904: 0.3844825029373169\n",
            "Loss @ time 905: 0.3847337067127228\n",
            "Loss @ time 906: 0.38284972310066223\n",
            "Loss @ time 907: 0.3794397711753845\n",
            "Loss @ time 908: 0.375914603471756\n",
            "Loss @ time 909: 0.3738114535808563\n",
            "Loss @ time 910: 0.37308940291404724\n",
            "Loss @ time 911: 0.37355583906173706\n",
            "Loss @ time 912: 0.37414249777793884\n",
            "Loss @ time 913: 0.3740089535713196\n",
            "Loss @ time 914: 0.37298384308815\n",
            "Loss @ time 915: 0.37096136808395386\n",
            "Loss @ time 916: 0.368539959192276\n",
            "Loss @ time 917: 0.3666016757488251\n",
            "Loss @ time 918: 0.36593005061149597\n",
            "Loss @ time 919: 0.36635711789131165\n",
            "Loss @ time 920: 0.36810222268104553\n",
            "Loss @ time 921: 0.3698406517505646\n",
            "Loss @ time 922: 0.37066948413848877\n",
            "Loss @ time 923: 0.37012040615081787\n",
            "Loss @ time 924: 0.3686682879924774\n",
            "Loss @ time 925: 0.3667118549346924\n",
            "Loss @ time 926: 0.36494362354278564\n",
            "Loss @ time 927: 0.36363688111305237\n",
            "Loss @ time 928: 0.36306336522102356\n",
            "Loss @ time 929: 0.3630589246749878\n",
            "Loss @ time 930: 0.36350199580192566\n",
            "Loss @ time 931: 0.36411410570144653\n",
            "Loss @ time 932: 0.3647727966308594\n",
            "Loss @ time 933: 0.36499395966529846\n",
            "Loss @ time 934: 0.3651399612426758\n",
            "Loss @ time 935: 0.36488762497901917\n",
            "Loss @ time 936: 0.3644517660140991\n",
            "Loss @ time 937: 0.3635820150375366\n",
            "Loss @ time 938: 0.3626139163970947\n",
            "Loss @ time 939: 0.3618329167366028\n",
            "Loss @ time 940: 0.36131903529167175\n",
            "Loss @ time 941: 0.3611733615398407\n",
            "Loss @ time 942: 0.36124566197395325\n",
            "Loss @ time 943: 0.3616979718208313\n",
            "Loss @ time 944: 0.3621433973312378\n",
            "Loss @ time 945: 0.3626730740070343\n",
            "Loss @ time 946: 0.36311227083206177\n",
            "Loss @ time 947: 0.36338910460472107\n",
            "Loss @ time 948: 0.36336639523506165\n",
            "Loss @ time 949: 0.36303094029426575\n",
            "Loss @ time 950: 0.36244186758995056\n",
            "Loss @ time 951: 0.3618461489677429\n",
            "Loss @ time 952: 0.3614509105682373\n",
            "Loss @ time 953: 0.36130180954933167\n",
            "Loss @ time 954: 0.3611496686935425\n",
            "Loss @ time 955: 0.3609902262687683\n",
            "Loss @ time 956: 0.3608935475349426\n",
            "Loss @ time 957: 0.36082935333251953\n",
            "Loss @ time 958: 0.36068013310432434\n",
            "Loss @ time 959: 0.36051544547080994\n",
            "Loss @ time 960: 0.3602868616580963\n",
            "Loss @ time 961: 0.3600997030735016\n",
            "Loss @ time 962: 0.3599555194377899\n",
            "Loss @ time 963: 0.359836220741272\n",
            "Loss @ time 964: 0.3597758412361145\n",
            "Loss @ time 965: 0.3598408102989197\n",
            "Loss @ time 966: 0.35986900329589844\n",
            "Loss @ time 967: 0.35987913608551025\n",
            "Loss @ time 968: 0.359927773475647\n",
            "Loss @ time 969: 0.3600849211215973\n",
            "Loss @ time 970: 0.3604525625705719\n",
            "Loss @ time 971: 0.3610638678073883\n",
            "Loss @ time 972: 0.3617582321166992\n",
            "Loss @ time 973: 0.3629729747772217\n",
            "Loss @ time 974: 0.36435258388519287\n",
            "Loss @ time 975: 0.3663489818572998\n",
            "Loss @ time 976: 0.36826324462890625\n",
            "Loss @ time 977: 0.3701612949371338\n",
            "Loss @ time 978: 0.3708850145339966\n",
            "Loss @ time 979: 0.3703075051307678\n",
            "Loss @ time 980: 0.3675546646118164\n",
            "Loss @ time 981: 0.36351147294044495\n",
            "Loss @ time 982: 0.3596404790878296\n",
            "Loss @ time 983: 0.3573344349861145\n",
            "Loss @ time 984: 0.35713374614715576\n",
            "Loss @ time 985: 0.35855650901794434\n",
            "Loss @ time 986: 0.3605751693248749\n",
            "Loss @ time 987: 0.36218398809432983\n",
            "Loss @ time 988: 0.36295920610427856\n",
            "Loss @ time 989: 0.36259910464286804\n",
            "Loss @ time 990: 0.3612561821937561\n",
            "Loss @ time 991: 0.35938403010368347\n",
            "Loss @ time 992: 0.3577246069908142\n",
            "Loss @ time 993: 0.356715589761734\n",
            "Loss @ time 994: 0.35640189051628113\n",
            "Loss @ time 995: 0.356708824634552\n",
            "Loss @ time 996: 0.3572048246860504\n",
            "Loss @ time 997: 0.35775452852249146\n",
            "Loss @ time 998: 0.35800039768218994\n",
            "Loss @ time 999: 0.3578593134880066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md4TtaoW1CyG"
      },
      "source": [
        "## Evaluating our Model\n",
        "\n",
        "Now that we've trained our network, let's look at how well we did. To do this, we will feed in a range of theta values and see how well our networks predictions line up with the ground truth dynamics of our system. We plot the original data in blue, and our neural networks fit in red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kep9pA0Vsh8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "b93a5be5-d687-4d75-958f-0c5bd3a0addd"
      },
      "source": [
        "if optimizer is not None and L is not None:\n",
        "  theta_eval = torch.unsqueeze(torch.linspace(-3*np.pi, 3*np.pi, 10000), dim=1)\n",
        "  theta_ground = np.linspace(-2*np.pi, 2*np.pi, 100)\n",
        "  theta_dot_ground = u - m*g*l*np.sin(theta_ground)\n",
        "\n",
        "  # NOTE: some layers like batchnorm or dropout layers need to be turned\n",
        "  # off during evaluation and turned on during training. Calling model.eval()\n",
        "  # will do this for you. We do not use these types of layers here, but we\n",
        "  # call eval() here to demonstrate for potential future use. To go back to \"training\"\n",
        "  # mode, call model.train()\n",
        "  NN.eval()\n",
        "\n",
        "  # evaluate our network\n",
        "  theta_dot_eval = NN(theta_eval)\n",
        "\n",
        "  # plot our fit\n",
        "  plt.scatter(theta_ground, theta_dot_ground, s=6)\n",
        "  plt.plot(theta_eval, theta_dot_eval.data.numpy(), color='red', lw=1) # this is how to convert a pytorch tensor to a numpy array\n",
        "  plt.plot(np.linspace(-4*np.pi, 4*np.pi, 3), np.zeros(3), color='black', lw=1)\n",
        "  plt.xlim(-3*np.pi, 3*np.pi)\n",
        "  plt.xlabel('theta')\n",
        "  plt.ylabel('theta_dot')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44cLk3K6x4ZY"
      },
      "source": [
        "## Written questions\n",
        "\n",
        "Answer these written exercises in a pdf (preferably latex) and upload it gradescope.\n",
        "\n",
        "1) Within the range $\\theta \\in [-5.0, 5.0]$, which values of $(\\theta, \\dot \\theta)$ represent fixed points? Which fixed points are stable and which are unstable?\n",
        "\n",
        "2) What is one reason why our model might not fit the ground truth data exactly?\n",
        "\n",
        "3) Does our model give us a reasonable approximation of the dynamics for all $(\\theta, \\dot \\theta)$ pairs? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-T-a5gAh3OH"
      },
      "source": [
        "## How Will this Notebook Be Graded?\n",
        "If you are enrolled in the class, this notebook will be graded using [Gradescope](https://www.gradescope.com).\n",
        "We will send you the details of how to access the course page in Gradescope by email.\n",
        "\n",
        "We will replicate your work by running your notebook and checking that the final loss value is within a reasonable range.\n",
        "\n",
        "You will get full score if the following test succeeds:\n",
        "- Training runs and the final loss value is within a reasonable range of the value we are generating.\n",
        "\n",
        "This should hold if you define your network appropriately, and choose the right optimizer/loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgUjMUG_h3OI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf677d04-1ee0-4045-f252-50355b8d374b"
      },
      "source": [
        "from underactuated.exercises.pend.learning_dynamics.test_learning_dynamics import TestLearningDynamics\n",
        "from underactuated.exercises.grader import Grader\n",
        "Grader.grade_output([TestLearningDynamics], [locals()], 'results.json')\n",
        "Grader.print_test_results('results.json')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total score is 5/5.\n",
            "\n",
            "Score for Test dynamics model training is 5/5.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcvCWLMPTHC_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}